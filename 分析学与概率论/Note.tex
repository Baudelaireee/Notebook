\documentclass[en,geye,blue,pc,12pt]{elegantnote}
\input{command.tex}

\title{Math remark
\\ fondation for analysis and probability
}

\author{X}
\institute{Elegant\LaTeX{} Program}

\begin{document}

\maketitle

\newpage

\tableofcontents

\newpage
%-----------------------------------------------------
\section{Genenralisation of several inequalities}
\subsection{Jense,Hölder,Minkeoski's inequalities}

The description of the Jense inequality depends on the properties of the convex function, it is a strong inequalities which can be applied in many places. We should pay a little attention to the outline of the section:
\[\text{Jense} \Rightarrow \text{Hölder} \Rightarrow \text{Minkeoski}\]

\begin{theorem}[Jense]$ \\$
    Suppose \(\varphi: I \to \rr \) is a convex function defined on an interval,
    \(\msp\) is a probability space and \(f \in L^1(X)\) with \(imf \subset I\), then \(\int_X f d\mu \in I\) and \(\varphi \circ f\) is integrable such that 
    \[\varphi(\int_X f d\mu) \leq \int_X \varphi \circ f d\mu\]
    The equality holds iff \(f\) is constant almost everywhere.

    \begin{proof}
        
    \end{proof}
\end{theorem}

\begin{remark}
    There are many other forms of the Jense's inequality, we take some exemples.
    \begin{itemize}
        \item \textbf{Finite forms}: the motivation of the inequality comes from the definition of the convex function, i.e. the real-valued function satisfies
        \[f(tx+(1-t)y) \leq tf(x) + (1-t)f(y)\]
        for any defined \(x,y\) and \(t \in [0,1]\), here a question about the distribution of the weights appears, which is the core of the convex function. we can generalize the inequalities by appling the weights to the n different points in interval such that \(\sum_i w_i =1\), then we can conclude the inequality:
        \[f(\sum_i w_i x_i) \leq \sum_i w_i f(x_i)\]
        notice that the defined domaine usually is a convex set, which ensures the effectivity of \(f(\sum_i w_i x_i)\). 

        \item \textbf{Expectation:} By a simple change of the notation, the Jense inequality in a probaility space can be wriiten as the form:
        \[\varphi(E[X]) \leq E[\varphi(X)]\]
        Applying a classic convex function \(t \mapsto t^2\) we can get the important inequality in probability:
        \[E^2[X] \leq E[X^2]\]
        \item \textbf{Concave:} some function like \(t \mapsto lnt\) is a concave function, the Jense's inequality can be just changed the order of the inequality. The reason is simple, if \(f\) is a concave function , then \(-f\) will be a convex function.
    \end{itemize}
\end{remark}

The classic proof of the Hölder's inquality covers the inequality of Young:
\[ab \leq \frac{a^p}{p}+\frac{b^q}{q}\]
for any \(a,b \geq 0\) and \(p,q>1\) such that \(1/p+1/q=1\). The complete proof can be found in \textbf{[Rudin 1 Ex 6.10]}. I don't choose the proof here for a comparison of strength of the different inequality. Here the proof is elegant and given by Mon.Mardare in TD, and  a similar proof via Jense can be found in \textbf{[SU TD1 EX15-16]}.
\begin{theorem}[Hölder]$ \\$
    Suppose that \(p,q >1\) and \(1/p+1/q=1\), for any two mesurable functions \(f, g: \msp \to \cc\), we have 
    \[\int_X |fg| d\mu \leq (\int_X |f|^p d\mu)^{1/p}(\int_X |g|^q d\mu)^{1/q}\]
    the equality holds iff \(|f| = c|g|\) almost everywhere (u-p.p) for some constant \(c\).

    \begin{proof}
        Let \(u = \frac{|f|}{\|f\|_p}\) and \(v = \frac{|g|}{\|g\|_q}\), then notice that \(\|u\|_p = \|v\|_q = 1\). We know \(t \mapsto lnt\) is a concave function on \((0,+\infty)\), so we can estimate by Jense's inequality
        \begin{align*}
            ln(uv) = ln(u+v) &= \frac{1}{p}lnu^p+\frac{1}{q}lnv^q \\
            &\leq ln(\frac{1}{p}u^p+\frac{1}{q}v^q)
        \end{align*}
        by the monotone of the function, we have \(uv \leq \frac{1}{p}u^p+\frac{1}{q}v^q\), hence we can conclude
        \begin{align*}
            \frac{\int_X |fg| d\mu}{(\int_X |f|^p d\mu)^{1/p}(\int_X |g|^q d\mu)^{1/q}} &=  \int_X uv d\mu \\
            &\leq \frac{1}{p}\int_X u^p d\mu + \frac{1}{q}\int_X v^q d\mu \\
            & = \frac{1}{p}\|u\|_p^p+\frac{1}{q}\|v\|_q^q = 1
        \end{align*}
        finally, we discuss the equality. By Jense, we know the equality holds iff \(u = v\). Notice that if \(u=0\) or \(v=0\) almost everywhere, then the inequality can be reduced to \(0 \leq 0\), it holds, otherwise we can get that \(|f| = \frac{\|f\|_p}{\|g\|_q}|g|\), so we finish the proof.
    \end{proof}
\end{theorem}

\begin{corollary}[Cauchy-Schwartz]$ \\$
    For any two square integrable function \(f,g \in L^2_\cc(\rr^n)\), we have 
    \[|\int_{\rr^n} f\overline{g}dl|^2 \leq \int_{\rr^n}|f|^2dl \cdot \int_{\rr^n}|g|^2dl \]
    The equality holds iff \(|f| = c|g|\) almost everywhere (u-p.p) for some constant \(c\).

    \begin{proof}
        Although it is the special case of Hölder when \(p = q =2 \), but usually in a Hilbert space we have the beautiful form as following
        \[|<u,v>| \leq \|u\|\|v\|\]
        the inequality has a good geometric intuition, and the proof of it is very beautiful and elementry. Notice \(<u,v> \in \cc\), so there exists \(z \in \cc\) such that \(|z| =1\) and \(z<u,v>=|<u,v>|\), and we let \(p(t) = <tzu+v,tzu+v>\) defined on \(\rr\), then 
        \begin{align*}
            p(t) &= t^2z\overline{z}<u,u>+tz<u,v>+t\overline{z}<v,u>+<v,v> \\ 
            &= t^2z\overline{z}<u,u> + tz<u,v>+t \overline{z<u,v>}+<v,v> \\
            &= t^2\|u\|^2+2|<u,v>|t+\|v\|^2
        \end{align*}
        so \(p(t)\) can be arranged to be a quadratic polynomial with respect to real value \(t\), and \(p(t) = \|tzu+v\|^2 \geq 0\), so we have suiffsant and necessary condition that 
        \[\Delta  = 4|<u,v>|^2- 4\|u\|^2\|v\|^2 \geq 0\]
        which is the inequality we hope to get, and \(\Delta = 0\) happens iff the polynomial satisfies \(p(t)=(t\|u\|+\|v\|)^2 = 0\).
    \end{proof}
\end{corollary}

\begin{theorem}[Minkeoski]$ \\$
    For any \(p \geq 1\), suppose that \(f,g: \msp \to \cc\) are two mesurable functions, then we have 
    \[\int_X |f+g|^p d\mu \leq \int_X |f|^p d\mu +\int_X|g|^pd\mu\]
    The equality holds iff \(|f| = c|g|\) almost everywhere (u-p.p) for some constant \(c\).

    \begin{proof}
        If 
    \end{proof}
\end{theorem}

Review some basic inequalities (discrete)...
\subsection{Markov, Tchebychev, Cantelli's inequalities}
This section covers some basic inequalities in properties, They are always very useful when estimation. And in this section ,we always use \(\psp\) to denote a probability space

\begin{theorem}[Markov]$ \\$
    Suppose that \(Y: \Omega \to \rr^+\) is a random variable, then for any \(t > 0\), we have 
    \[P(Y \geq t) \leq \frac{E(Y)}{t}\]

    \begin{proof}
        We notice that \(t \cdot \cha_{\{Y\geq t\}}\leq Y\), it is easily to prove. If \(x \in \{Y \geq t\}\), then \(t \cdot \cha_{\{Y\geq t\}}(x) = t \leq Y(x)\), otherwise \(t \cdot \cha_{\{Y\geq t\}}(x) = 0 \leq Y(x)\), so by monotone of the expectation
        \[E[t \cdot \cha_{\{Y\geq t\}}] = tE[t \cdot \cha_{\{Y\geq t\}}] = tP(Y \geq t)\]
    \end{proof}
\end{theorem}

\begin{theorem}[Tchebychev]$ \\$
    Suppose that \(Y: \Omega \to \rr\) is a random variable in \(L^2(\Omega)\) (square integrable), then for any \(t>0\) we have 
    \[P(|Y-E[Y]| \geq t ) \leq \frac{V(Y)}{t^2}\]

    \begin{proof}
        We apply Markov to prove it, we can estimate 
        \begin{align*}
            P(|Y-E[Y]| \geq t) &= P((Y-E[Y])^2 \geq t^2) \\
            & \leq \frac{E[(Y-E(Y))^2]}{t^2} \\
            &= \frac{V(Y)}{t^2}
        \end{align*}
    \end{proof}
\end{theorem}

\begin{theorem}[Cantelli]$ \\$
    Suppose that \(Y: \Omega \to \rr\) is a random variable in \(L^2(\Omega)\) (square integrable), then for any \(t>0\) we have 
    \[P(Y-E[Y] \geq t ) \leq \frac{V(Y)}{V(Y)+t^2}\]
    \[P(|Y-E[Y]| \geq t ) \leq \frac{2V(Y)}{V(Y)+t^2}\]
\end{theorem}

\begin{proof} 
    The proof of the inequality need to use inequality: for any \(a,b>0\), 
    \begin{equation}
        P(X \geq a) \leq \frac{E[(X+b)^2]}{(a+b)^2}\label{Cantelli}
    \end{equation}
    We write \(P(X \geq a) = E[ \cha_{\{X \geq a\}}]\), then we just need to prove \(\cha_{\{X \geq a\}} \leq (\frac{X+b}{a+b})^2\).
    For any \(w \in \{X \geq a\}\), we have \((\frac{X+b}{a+b})^2(w) \geq (\frac{a+b}{a+b})^2 = 1 =\cha_{\{X \geq a\}}(w)\), otherwise will conclude \((\frac{X+b}{a+b})^2 \geq 0\), so the inequality \ref{Cantelli} is valid.

    For proove the Cantelli, we let \(Z = \frac{Y-E[Y]}{\sigma(Y)}\), then by above inequality
    \begin{align*}
        P(Y \geq t) &\leq \frac{E[(Y+b)^2]}{(t+b)^2} \\
        &= \frac{E[Y^2]+2bE[Y]+b^2E[1]}{(t+b)^2} \\
        &= \frac{1+b^2}{(t+b)^2}
    \end{align*}
    We can put \(b = 1/t\), then \(\frac{1+b^2}{(t+b)^2} = \frac{1}{1+t^2}\), and then rewrite the inequality we get 
    \(P(Y-E[Y] \geq t) \leq \frac{V(Y)}{V(Y)+t^2}\).
    and notice that 
    \[P(Y-E[Y] \leq -t) = P((-Y)-E[-Y] \geq t)\]
    and \(V(-Y)=V(Y)\) implies the same upper bound, so we finish the proof.
\end{proof}

The importance of the three inequalities is the estimation of the upper bound of some probability, if we can know the the expected value, we can get an upper bound by Markov; if we also know the variance, a more precise upper bound can be given by Tchebychev and Cantelli.

Cantelli's inequality is a work based on the Tchebychev's inequality, if we rewrite the Tchebychev's inequality by take \(t = \sigma(Y)t\), then we have
    \[P(|X-\mu|\leq \sigma t) \geq \frac{1}{t^2} \label{eq:Tchebychev}\]
Which always appears in the some statistical object like confidence interval, but it is a two-side interval estimation. If we rewrite Cantelli's inequality similarly:
\[P(X-\mu\leq \sigma t) \geq \frac{1}{1+t^2}\]
\[P(|X-\mu|\leq \sigma t) \geq \frac{2}{1+t^2}\]
We can find that Cantelli's inequality can do one-side estimation and it provides a worse bound then Tchebychev's when \(t \geq \sigma\).

\section{Classic anaylsis theory and technic}
\subsection{Power series and analytic function}

This section conatains Three parts:
\begin{itemize}
    \item overview for power series and the important result
    \item real analytic function and Abel's theorem
    \item the superiority of the holomorphic function
\end{itemize}
Power series is an special case of the series of the function, it gives by the follwoing sequence:
\[f_n: D \to \cc, \quad z \mapsto a_nz^n\]
more general, we can write it as the form \(a_n(z-z_0)^n\), but notice that we can just change variable by \(u=z-z_0\), it is a translation, so clearly a diffeomorphism. In above statement \(D\) denote the domaine where the series will be well-defined, i.e. convergent, so here needs a clear definition.

\begin{lemma}[Abel's Lemma]$ \\$
    Given a complex-valued sequence \((a_n)_{n \in \n}\), if there exists \(z_0 \in \cc\) such that \((a_nz_0^n)_{n \in \n}\) is bounded, then we can conclude that\\
    -If \(|z| < |z_0|\), then power series \(\powerseries\) converge.\\
    -If \(|z| < |z_0|\), then power series \(\powerseries\) diverge.\\
    \begin{proof}
       The bounded sequnce implies a real number \(M\) such that \(a_nz_0^n \leq M\), so 
       \[|a_nz^n| = |a_nz_0^n \cdot \frac{z^n}{z^n_0}| \leq M|\frac{z}{z_0}|^n\]
       so clearly the power series converge if \(|z| \leq |z_0|\) by M-test. 

       If \(|z| > |z_0|\), then \((a_nz^n)_{n \in \n}\) is not bounded, so immediately it cant converge to zero, so the power series diverge.
    \end{proof}
\end{lemma}

The lemma inspire us that the domaine is nearly a disk, although we can not determine the convergence of the boundary, but we can formally define the \textbf{radius of convergence} of a power series \(\powerseries\) is given by
\[R = \sup \{r \geq 0 |(a_nr^n)_{n \in \n} \text{ is bounded}\}\]
and we call \(D = D(0,R)\) the \textbf{domaine of convergence}. A more simple form of the raduis of convergence is given by Cauchy and d'Alembert:
\[1/R = \limsup |a_n|^{1/n} = \lim |\frac{a_{n+1}}{a_n}|\]
and we convented here \(1/0 = \infty\) and \(1/\infty = 0\).

The convergence of the power series provides a good case of function, analytic function, which is the theme of the next section. We put function
\[f:\overline{D(0,r)} \to \cc, \quad d z \mapsto \powerseries\]
where \(r \in (0,R)\), and we suppose here \(R>0\). The function will be well-defined and continous since \(z \mapsto a_nz^n\) is always uniformly continous on a compact set, so we can conclude that \(f\) is well-defined and continous on domaine convergence when we take \(r \rightarrow R\). We will take about the derivable of the power series, it is a important property.

\begin{theorem}[Derivable]$ \\$ \label{derivable}
    The power series \(f(z) = \powerseries\) defined on the domaine \(D(0,R)\) with raduis \(R >0\), then the derivative of \(f\) is also a power sereies  with the same raduis of convergence and has the form
    \[f'(z) = \sum_{n \in \n} na_nz^{n-1}\]

    \begin{proof}
        
    \end{proof}
\end{theorem}

An immediate result is that power series is a holomorphic function, and another result is an inversion of the process, that means if we know a function definitely is converged by some power series in some domaine, then we can write each coefficient in terms of the derivative of the function, usually in calculus we call it the Taylor's Formula.
\begin{corollary}[Taylor's series]$ \\$
    If \(f(z)\) is the sum of the power series \(\powerseries\) on the domaine \(D(0,R)\) with \(R>0\), then we have
    \[a_n = \frac{f^{(n)}(0)}{n!}.\]
    
    \begin{proof}
        Manipulate above theorem to \(f\) k times we get
        \[ f^{(k)}(x) = \sum_{n \in \n}n(n-1)\cdots(n-k+1)a_nz^{n-k}\]
        and we put \(z = 0 \), then we get \(n!a_n = f^{(k)}(0)\), that is what we want.
    \end{proof}
\end{corollary}

we can also conclude the \textbf{analytic continuation} about the power series, although it can be got direct from holomorphic function, but the proof here is easy and it gives us the connection.
\begin{corollary}
    Suppose that \(\powerseries\) and  \(\powerseriesb\) are two power series with the positive raduis \(R_1\) and \(R_2\), if there exists an voisinage of zero \(V\) such that \(\powerseries = \powerseriesb\), then \(a_n = b_n\) for any \(n \in \n\). 

    \begin{proof}
        We Let \(f(z) = \powerseries\) and \(g(z) = \powerseriesb\) on domaine, then we have 
        \[0 = (f-g)(z) = \sum_{n \in \n} (a_n-b_n)^nz^n\]
        on the subset of the domaine \(V\), so we applying above theorem we can get that \[a_n-b_n = n!(f-g)(0) = 0\] Which is the result we get,
    \end{proof}
\end{corollary}

After some basic description of power series, we can define the \textbf{analytic function}, 
\begin{definition}
    Let \(\kk = \rr\) or \(\cc\), \(U\) is a open set in \(\kk\), a function\(f: U \to \kk\) is an analytic function if for any \(z_0 \in U\), the function can be converged by a power series in a small voisinage of \(z_0\).

    formally speaking, for any \(z_0 \in U\), there exists \(r > 0\) and a sequence \((a_n)_{n \in \n}\) such that in the domaine \(D(z_0,r) \subset U\) we have
    \[f(z) = \sum_{n \in \n} a_n(z-z_0)^n\]
\end{definition}

\begin{remark}
    a power series defined on a domain is clearly an analytic function, the choice of \(r\) can be formulated by \(\min\{|z_0|,R-|z_0|\}\), where \(R\) is the raduis of convergence.
\end{remark}

The classic discussion about the analytic function is that whether  analytic is a smooth function(\(C^{\infty}\)), and conversely a smooth function is a analytic function? In the time of Abel, analysis is focused on \(\rr\), and a smooth function is usually considered to do expansion by Talyor's Formula, so the question in \(\rr\) is equivalent to the question that for each smooth function whether its Taylor sereies converges to itself or not. In conclusion, the relation show as folloing in \(\rr\)
\[ \text{analytic function} \subset C^{\infty} \text{smooth function}\]

\begin{proposition}[infinitely differentiable]$ \\$
    For any analytic function defined on an open set of \(\kk\), it is infinitely differentiable and its derivative is still an analytic function.

    \begin{proof}
        We let \(f: U \to \kk\), and take \(a \in U\), the analytic function implies a positive \(r>0\) and a seuqnce \((a_n)\), such that \(f(z) = \sum a_n(z-a)^n\) for any \(z \in D(a,r)\), then applying theorem \ref{derivable} we can conclude that 
        \[f^{(k)} = k!a_k\]
        for any \(k \in \n\), hence it is infinitely differentiable. What's more, for any \(z \in D(a,r)\), \(f'(z) = \sum na_n(z-a)^{n-1}\) by the same theorem, which gurantee that the derivative is still an analytic function.
    \end{proof}
\end{proposition}

Now we will give an example that a real-value function is smooth but not analytic to finish our conclusion.

\begin{example}
    we consider the following real function
    \[f: \rr \to \rr, \quad x \mapsto \begin{cases}
        e^{-1/x^2} \quad &x \neq 0 \\
        0 \quad &x = 0
    \end{cases}\]
    Notice that \(x \mapsto e^{-1/x^2}\) is a \(C^{\infty}\) function for any \(x \neq 0\), and we can prove by recurrence that its n-times derivative has the form 
    \[f^{(n)}(x) = \frac{P_n(x)}{x^{3n}}e^{-1/x^2}\]
    where \(P_n \in \rr[x]\). Then calculate the limite by changing \(u = 1/x\), we know
    \[\lim_{x \rightarrow 0}\frac{e^{-1/x^2}}{x^{3n}} = \lim_{u \rightarrow \infty}\frac{u^{3n}}{e^{u^2}} = 0\]
    so for any \(n \in \n\), \(f^{(n)}(0)\) tends to zero, so we expend the function by adding \(f^{(n)}(0) = 0\) such that \(f \in C^{\infty}(\rr)\).

    Suppose that \(f\) is analytics at \(x=0\), then we can write the Talyor's series at the voinsinage of zero
    \[f(x) = \sum_{n \in \n}\frac{f^{(n)}(0)}{n!}x^n = 0\]
    which means the function equals to null in the voisinage, clearly absurd since the series is not convergent to itself. Hence, f can not be analytic at zero although it is a smooth function.
\end{example}

Another work of Abel is about the convergence on the boundary of domaine of convergence, we give it here the version in \(\rr\).
\begin{theorem}[Abel's Theorem]$ \\$
    Let \(f(z) = \powerseries\) be a power sereies defined on a domaine \((-R,R)\) and \(R>0\). If the power series converges at \(z= R\)(or \(z=-R\)), then the \(f\) is continous (one-sid) at \(z= R\)(or \(z=-R\)). Or simply, the theorem is equivalent to porve that 
    \[\lim_{r \rightarrow 1^-} \sum_{n \in \n}a_nr^n =\sum_{n \in \n}a_n \]
    if \(\sum_{n \in \n}a_n\) is a covergent series.

    \begin{proof}
        the rigorous proof needs to use the \textbf{summation by parts formula}:
        \[\sum_{n=M}^N a_nb_n = a_NB_N-a_MB_{M-1}-\sum_{n =M}^{N-1}(a_{n+1}-a_n)B_n \]
        where \(a_n\) and \(b_n\) are all finite, and \(B_n = \sum_{k=1}^{n}b_k\) with convention \(B_0 = 0\).
    \end{proof}
\end{theorem}

Finally i will talk about holomorphic function. Loosely speaking, homomorphic function is just the differnetiable functions in \(\cc\), why do we not just call them "a differentaible function" and give them a special name? The important reason is the crazy regularity of the complex-valued function. By \textbf{Cauchy's integral formula}, for any open set containing an open disc \(D\), if a function \(f\) is holomorphic in the open set, then for any \(z \in D\)
\[f(z) = \frac{1}{2 \pi i} \int_{\partial D}\frac{f(\zeta )}{\zeta - z} d\zeta\]
then by caculation
\[\frac{f(z+h)-f(z)}{h} = \frac{1}{2 \pi i }\int_{\partial D}\frac{f(\zeta )}{(\zeta - z)(\zeta -z -h)} d\zeta\]
By the continuity of the parametric integration at \(h =0\), we can conclude that \(f\) has derivatice and \(f(z) = \frac{1}{2 \pi i }\int_{\partial D}\frac{f(\zeta )}{(\zeta - z)^2} d\zeta\), hence we can prove by recurrence that \(f\) is infinitely times differnetiable and with the form
\begin{equation}
    f^{(n)}(z) = \frac{n!}{2 \pi i }\int_{\partial D}\frac{f(\zeta )}{(\zeta - z)^{n+1}} d\zeta \label{eq:derivative of holomorphic function}
\end{equation}

That is an amazing result and also a fundamental theorem for complex analysis, so clearly, any holomorphic function is a \(C^{\infty}\) function, also the most important points is that the high-order derivative only depends on the original function. We recall the theorem \ref{derivable}, then we can complete the connection between analytic function and a smooth function in \(\cc\): 
\[\text{analytic function} \iff \text{holomorphic function}\]

\begin{theorem}[Stein 4.4]$ \\$
    If \(f\) is holomorphic function in an open set \(\Omega \subset \cc\), then \(f\) is an analytic function in \(\Omega\).

    \begin{proof}
        By definition, we just need to verify that in any \(z_0 \in \Omega\), there exists a voisinage such that the Talyor's sereies \(\sum_{n \in \n }\frac{f^{(n)}(z_0)}{n!}(z-z_0)^n\) converges to \(f\).

        Too see that we notice in Cauchy integration formula
        \[\frac{1}{\zeta - z} = \frac{1}{\zeta - z_0} \frac{1}{1-u}\]
        by taking \(u = \frac{z-z_0}{\zeta - z_0}\), then for any \(|u| <1\), we can rewrite formula as the form
        \[f(z) = \frac{1}{2 \pi i }\int_{\partial D}\frac{f(\zeta)}{\zeta - z_0} \sum_{n \in \n}u^n d \zeta = \sum_{n \in \n}\frac{(z-z_0)^n}{2 \pi i}\int_{\partial D}\frac{f(\zeta)}{(\zeta-z_0)^{n+1}} d \zeta \]
        Here interchange \(\int / \sum\) can be done since the holomorphic function is continous. and take equation \ref{eq:derivative of holomorphic function} into above equality then we can get the result we hope.
    \end{proof}
\end{theorem}

\begin{remark}
    That means for any holomorphic function, we can do expanasion casually.
\end{remark}

















\end{document}